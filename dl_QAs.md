## Q1:批量计算出的梯度各不相同，为什么平均梯度可以代表正确的更新方向？

### 1. 大数定律与期望估计

**核心思想：单个样本的梯度是噪声估计，批量平均是更可靠的期望估计**

- 每个样本的梯度 ∇Lᵢ(W) 可以看作是真实**期望梯度**的一个带噪声的估计
- 根据大数定律，当样本量足够大时，样本均值会收敛于总体期望值
- 平均梯度：`(Σ∇Lᵢ(W))/n ≈ E[∇L(W)]`

### 2. 减少随机噪声

在深度学习中，数据通常包含：

- **信号**：与任务真正相关的模式
- **噪声**：采样误差、标注错误、无关特征等

单个样本的梯度中既包含信号也包含大量噪声。当对批量梯度求平均时：

- **信号部分**：倾向于在相同方向叠加（因为数据有共同模式）
- **噪声部分**：倾向于在不同方向相互抵消

**结果**：信噪比提高，平均梯度更接近真实的优化方向。

### 3. 对于权重的更新

假设：

- 权重参数为 `W`
- 损失函数为 `L`
- 批量大小为 `batch_size`，即一个批量包含 `n` 个样本
- 学习率为 `α`

**步骤 1：前向传播**
将整个批量的 `n` 个样本一次性输入网络，计算出每个样本的损失值 `L_i`（其中 `i` 从 1 到 `n`）。

**步骤 2：反向传播**
计算每个样本损失 `L_i` 对于权重 `W` 的梯度 `∇L_i(W)`。现在你得到了 `n` 个梯度值。

**步骤 3：聚合梯度（计算平均梯度）**
这是最关键的一步。我们将这 `n` 个梯度求平均值，得到一个代表整个批量方向的“平均梯度”。

```
平均梯度 = (∇L₁(W) + ∇L₂(W) + ... + ∇L_n(W)) / n
```

**步骤 4：更新权重**
使用这个平均梯度，按照梯度下降公式更新权重。

`W_new = W_old - α * (平均梯度)`
即：
`W_new = W_old - α * [ (∇L₁(W) + ∇L₂(W) + ... + ∇L_n(W)) / n ]`

#### 在代码中是如何实现的？

在现代深度学习框架（如PyTorch, TensorFlow）中，这个过程是自动完成的。

1. **前向传播**：你对一个批量数据 `input_batch` 进行预测，得到 `output_batch`。
2. **计算损失**：你调用 `loss = criterion(output_batch, target_batch)`。注意，这里的 `loss` 通常已经是**该批量内所有样本损失的平均值**。
3. **反向传播**：你调用 `loss.backward()`。框架会自动进行反向传播，并为每个参数 `W` 计算梯度，并累加到 `W.grad` 属性中。这个 `W.grad` 存储的**已经是整个批量的平均梯度**（因为损失是平均值，根据链式法则，得到的梯度自然也是平均值）。
4. **更新权重**：优化器（如 `optimizer.step()`）会执行 `W = W - α * W.grad`。
5. **清零梯度**：在下一个批量开始前，你必须调用 `optimizer.zero_grad()` 将 `W.grad` 重置为零，防止梯度累加到下一个批量。



## Q2：正则化

正则化（Regularization）是一种在训练机器学习模型时，在损失函数中添加额外项，来惩罚过大的参数，进而限制模型复杂度、避免过拟合，提高模型泛化能力的技术。

### L1正则化

L1正则化在损失函数中加入参数的绝对值之和：
$$
{Loss}_{L1}=原Loss+λ∑_{i=1}^k|ω_i |
$$
L1正则化通过惩罚模型参数的绝对值，使得部分权重趋近0甚至变为0。这会导致特征选择，即模型会自动“丢弃”一些不重要的特征。L1正则化有助于创建稀疏模型（即许多参数为0）。在解决回归问题时，使用L1正则化也被称为“Lasso回归”。

超参数控制着正则化的强度。较大的   值意味着强烈的正则化，会使模型更简单，可能导致欠拟合。而较小的   值则会使模型更复杂，可能导致过拟合。

### L2正则化

$$
Loss_{L2}=原Loss+λ∑_{i=1}^kω_i^2
$$

L2正则化通过惩罚模型参数的平方，使得所有参数都变得更小，但不会将参数强行压缩为0。它会使得模型尽量平滑，从而防止过拟合。在解决回归问题时，使用L2正则化也被称为“岭回归”

