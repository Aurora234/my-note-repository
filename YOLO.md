# YOLO 原文2016

## 引言  
模型检测很有用，一系列应用等待你解锁，当前的方法都是重复使用分类器的， 。为了检测一个物体，系统使用一个该物体的分类器在图片的不同位置检测
1. DPM方法，它使用滑动窗口
2. R-CNN分两阶段  
第一阶段从从图片中提取候选框  
第二阶段对每一个候选框进行分类  
然后进行后处理：去除重复的框，重新打分等

### 第一，YOLO非常快
YOLO直接把检测定义为一个回归问题，直接从输入图像的像素空间映射到分类结果，直接获得框的坐标和类别，皆可以实现目标分类也可是实现检测框的定位  

YOLO同时可以输出多个预测框和类别，在全幅图像上进行分类，直接端到端的优化  

因为把任务定义为一个端到端的回归问题，所以只需要一个模型即可。  
### 第二，YOLO输入全幅图片  
YOLO可以看见全局信息，R-CNN只能看到候选框内的信息，所以YOLO背景误判率很低，  
### 第三，YOLO泛化能力很强 
在自然图像上进行训练，艺术作品上进行测试，YOLO的效果远比DPM和R-CNN强的多。  
### 缺点  
在小目标和密集目标上的识别能力差  

要速度，准确率就低，反之亦然  

---

## 统一的目标检测框架
YOLO把一张图片划分为S*S的网格，如果一个物体的中心落到某个网格（cell）里面，那么该网格就负责预测该物体  

每一个网格预测B个Bounding boxes和这些box的置信度分数，这个分数不仅反应出这个box包含物体的信心有多大(非0即1)，还表示
预测的有多准确(IOU)。如果没有物体存在于某个网格，则置信度应该为0，否则他应该等于IOU  

每一个bounding box包含五个预测值（x，y，w,h,confidence）。x和y是这个box相对于所在cell左上角格点的坐标（x，y的值始终在0和1之间），同理w和h是相对于整幅图片的坐标，取值范围也是0-1。
confidence表示预测框和实际框的IOU。  

每一个cell也需要预测C个条件概率（cell在包含物体的条件下是某个类别的概率）。每个cell只生成一套条件概率，即使负责B个框（共享概率）。  

所以每一个cell预测C个条件概率和B个box。  

测试时，我们将条件概率和每一个框的confidence相乘得到每一个框各个类别的置信度分数。这个分数表明了回归定位的精度和分类的精度。  

所以系统输出一个S * S * (B * 5+C)的张量   

### 网络设计
使用卷积神经网络。最开始的卷积层负责提取特征，后面的全连接层负责预测上面说的数值。  
受GoogLeNet启发，有24个卷积层和2个全连接层，使用1 * 1后面跟上3 * 3卷积。

![yolo_network](https://raw.githubusercontent.com/Aurora234/picture_bed/main/yolo_v1/202511021141173.png)

### 训练  
我们在ImageNet上进行训练的时候，使用20个卷积层伴随着一个平均池化层，和一个全连接层。之后我们根据任少卿的理论
增加了4个卷积层和2个全连接层，使用的是权重随机初始化。我们把输入图像从224 * 224转换到448 * 448   

最终层预测那30个参数。我们用归一化将x，y，h，w映射到0-1之间  

我们用了leaky relu作为激活函数  

我们用平方和误差作为损失函数。平方和误差很容易优化，但是它与最大化平均精度不能充分吻合。因为平方和误差对定位和分类一视同仁，对所有的bounding box一视同仁，
而我们预测的时候会产生很多bounding boxes，但是
绝大多数的bounding boxes是无用的，这些box不预测物体，而且一个cell里面也有B-1个bounding box是不负责预测的，真正需要进行预测的bounding box
其实很少。所以我们要使那些不负责预测物体的bounding box对损失函数的影响尽量小，也就是减少它们在计算损失函数时的权重。  

我们加强定位误差，削弱不包含ground truth的预测值的confidence损失，权重分别设置为5和0.5。  

计算w和h时，平方和误差计算的是绝对值的误差，对大小框一视同仁，对大框不公平，因为同样的偏差，对于小框来说很敏感，而对于大框来说偏差几个像素问题不大。
所以要对预测值和真实值先去平方根，在进行平方和误差。使对小框的预测造成的损失函数值更大，需要尽快修正。

每个cell有多个box，在训练阶段我们只想要负责预测物体的那个box，就是那个IOU最大的box。如此，这些预测框就会特化，逐渐聚焦特定形状。提升recall  

**损失函数**  

![loss_function](https://raw.githubusercontent.com/Aurora234/picture_bed/main/yolo_v1/202511021141585.png)    

![image-20251102142922351](https://raw.githubusercontent.com/Aurora234/picture_bed/main/yolo_v1/202511021429496.png)

我们在Pascal Voc2007的测试集上进行训练，batch_size=64, 动量系数=0.9，正则化系数=0.0005  

对第一个epoch，学习率从10^-3到10^-2，之后用10-2训练75轮，10-3训练30轮，最后用10-4训练30轮  

为了避免过拟合，我们使用了dropout，系数为0.5，以广泛的数据增强，包括随机缩放，增加曝光度，HSV变换  

### 预测  

与训练一样，预测一张图片只需要一个网络结果，yolo非常快，因为需要一次前向推断的结果，不像其他的划窗方法。  

网格的设计是强制造成了空间上的差异，每一个网格强制负责预测某个地方的物体。会出现多个框预测同一个物体的情况，需要使用NMS非极大值抑制进行修正（杀死那些重复预测的框）  

### YOLO的缺陷  

YOLO强制空间限制，每一个cell只有两个box且只能有一个类别，所以限制了模型预测物体的个数，比如一群鸟的预测效果很差。  

虽然模型能够预测出边界框，但是在新的场景和不寻常比例物体预测上效果差。模型使用了粗粒度特征进行预测，因为使用了很多池化层，导致一些空间信息确实。  

模型对大小框同等对待，对于大框而言一点误差没问题，而对于小框而言一点误差会带来很大的影响。模型的主要误差来自于定位误差，分类正确但是定位误差大  

## 与其他模型比较  

### 与DPM比较  

DPM需要先设计好不同的模版，比如苹果模版，人类模版，而且人的动作也需要不同的模版，使用SVM分类器，预测时需要每一个模版在图像上滑动，不仅慢而且鲁棒性很差。YOLO使用端到端的模型，特征提取，box定位，NMS，上下文信息一次性完成。

### 与R-CNN比较  

R-CNN是region proposals方法的一个变种，先从图像中提取候选框，在逐一用卷积神经网络对每一个候选框进行特征提取，用SVM进行分类，然后在用线性模型进行回归，在用NMS去除重复的框。这每一个阶段都需要精细的调整，这样处理很慢。  

YOLO也是需要产生候选框，但是数量较少，可以避免一定的重复预测，之后也用CNN进行特征提取和分类，但是速度更快。

YOLO把所有流程整合到一起，所以比较快。

对单类别的预测比如人脸和行人是可以被高度优化的，因为不需要处理那么多的类别。YOLO是一个通用的目标检测模型。



## 实验



