

# 情感分析



## 基于同义词词典

**于同义词词典的方法**是情感分析中一种**基于规则**的经典方法。它通过“查字典”和“计分”的方式工作，具有简单、快速、无需训练的优点，但其准确性严重依赖于词典的质量和规则设计的完备性，在处理复杂语言时显得力不从心。在现代NLP中，它通常作为基线系统或辅助特征，与更先进的机器学习/深度学习方法结合使用。

### 核心思想

这种方法的核心思想非常简单直接：**通过查询一个预先构建好的、包含了词语情感极性的词典，来判断一段文本的情感倾向。**

它本质上是一种 **“查字典”** 的方法。这个字典不仅包含词语的同义词，更重要的是，它给每个词语打上了**情感标签**（例如，正面、负面、中性）或**情感分数**（例如，+3 表示非常正面，-2 表示比较负面）。

### 方法的工作流程

假设我们要判断句子 **“这部电影真棒，演员演技精湛，剧情却很枯燥。”** 的情感。

**第一步：构建情感词典（前提条件）**

我们需要一个情感词典。一个简单的情感词典可能长这样：

| 词语 | 情感极性 | 强度分数 |
| :--- | :------- | :------- |
| 棒   | 正面     | +3       |
| 精湛 | 正面     | +2       |
| 优秀 | 正面     | +2       |
| 枯燥 | 负面     | -2       |
| 糟糕 | 负面     | -3       |
| 美丽 | 正面     | +2       |
| 丑陋 | 负面     | -2       |
| ...  | ...      | ...      |

**第二步：文本预处理**

1. **分词：** 将句子分割成独立的词语。
   - `这 / 部 / 电影 / 真 / 棒 / ， / 演员 / 演技 / 精湛 / ， / 剧情 / 却 / 很 / 枯燥 / 。`
   - （注意：这里“真”和“很”是程度副词，需要特殊处理）

**第三步：查询词典并计算情感值**

1. 遍历每个分词后的词语，去情感词典里查找。
2. 找到匹配的词语，记录其情感分数：
   - `棒` -> +3
   - `精湛` -> +2
   - `枯燥` -> -2
   - 其他词语（如“电影”、“演员”、“剧情”）在词典中未找到，视为中性词，分数为0。
3. **（可选）处理程度副词和否定词：** 一个成熟的系统会考虑这些修饰词。
   - `真` 棒： “真”是加强语气的词，可以将“棒”的分数乘以一个系数（比如1.5）。所以 `棒` 的分数变为 +3 * 1.5 = +4.5。
   - `很` 枯燥：“很”也是加强词，`枯燥` 的分数变为 -2 * 1.2 = -2.4。
   - `却不枯燥`：如果遇到“不”这样的否定词，可以将其后词语的情感分数反转。`不枯燥` -> -(-2) = +2。

**第四步：汇总得分并得出结论**

- 将所有词语的情感分数相加：
  - 原始分数：+3 (棒) + +2 (精湛) + -2 (枯燥) = **+3**
  - 考虑程度副词后：+4.5 (真棒) + +2 (精湛) + -2.4 (很枯燥) = **+4.1**
- **制定判断规则：**
  - 如果总分 > 0，则判断为**正面**情感。
  - 如果总分 < 0，则判断为**负面**情感。
  - 如果总分 = 0，则判断为**中性**。

根据这个规则，我们的例子虽然提到了缺点（枯燥），但优点更突出，所以整体被判断为**正面评价**。

### 关键组成部分

一个完整的基于词典的系统通常包含以下几个词典：

1. **基础情感词库：** 核心词典，包含大量词语的情感极性和强度。
   - *例子：* `好 (+2)`, `坏 (-2)`, `美丽 (+2)`, `丑陋 (-2)`。
2. **否定词词典：** 包含可以反转情感倾向的词。
   - *例子：* `不`， `没`， `无`， `莫`。
3. **程度副词词典：** 包含可以加强或减弱情感强度的词，并为每个副词赋予一个权重。
   - *极强：* `极其`， `完美` (权重 ~2.0)
   - *强：* `非常`， `十分` (权重 ~1.5)
   - *一般：* `很`， `挺` (权重 ~1.2)
   - *弱：* `有点`， `稍微` (权重 ~0.5)
4. **连词词典：** 用于处理复杂句子结构，但简单实现中常被忽略。



## 基于计数的方法

### 核心思想

基于计数的方法的核心思想非常直观：**通过统计大规模文本数据中词语共同出现的频率，来量化它们之间的语义或语法关系。**

它的基本假设是 **“分布假说”**：**出现在相似上下文中的词语，具有相似的含义。** 例如，“咖啡”和“茶”经常出现在类似的语境中（如“喝”、“一杯”、“提神”），因此我们可以认为它们在语义上是相关的。

基于计数的方法旨在将文本中的词语转换为**数值向量**（即词向量），而这些向量的生成完全依赖于词语在语料库中的共现统计信息。

------

### 主要方法与工作流程

最经典的基于计数的方法是**词袋模型**及其扩展**TF-IDF**，以及用于生成稠密词向量的**共现矩阵与降维**技术。

#### 1. 词袋模型

这是最基础、最直观的基于计数的方法。

**工作流程：**

1. **构建词汇表：** 收集整个语料库（所有文档的集合）中所有不同的词语，形成一个词汇表。假设词汇表大小为 V。
2. **统计频率：** 对于每一篇文档，我们统计每个词语在文档中出现的次数。
3. **向量化：** 将每篇文档表示成一个长度为 V 的向量。向量的每一个位置对应词汇表中的一个词，其值就是该词在这篇文档中出现的次数。

**举例说明：**
假设我们的词汇表来自两个句子：

- 文档1: “我 喜欢 看 电影”
- 文档2: “我 喜欢 读 书”

**词汇表：** `[“我”， “喜欢”， “看”， “电影”， “读”， “书”]` (V=6)

| 词语 | 索引 |
| :--- | :--- |
| 我   | 0    |
| 喜欢 | 1    |
| 看   | 2    |
| 电影 | 3    |
| 读   | 4    |
| 书   | 5    |

现在，我们将两篇文档转换为向量：

- 文档1向量： `[1, 1, 1, 1, 0, 0]`
  - （“我”出现1次，“喜欢”1次，“看”1次，“电影”1次，“读”0次，“书”0次）
- 文档2向量： `[1, 1, 0, 0, 1, 1]`

**在电影评论分类中的应用：**

- 我们可以将每条电影评论看作一篇文档，用词袋模型将其转换为一个数值向量。
- 然后，将这些向量和它们对应的情感标签（正面/负面）输入到一个机器学习分类器（如朴素贝叶斯、逻辑回归、SVM）中进行训练。
- 分类器会学习到哪些词语的组合（即向量的模式）更倾向于正面评价，哪些更倾向于负面评价。

**缺点：**

- **高维稀疏：** 向量维度等于词汇表大小，可能高达数万甚至数百万，但每个文档向量中只有少量非零值。
- **忽略词序：** “电影不好看”和“不好看电影”的向量表示是完全一样的，但语义可能不同。
- **忽略语义：** 无法捕捉“电影”和“影片”之间的语义相似性，它们被视为两个完全独立的特征。

#### 2. TF-IDF

TF-IDF是词袋模型的增强版，它不仅考虑词频，还考虑词语在整个语料库中的重要性。

- **TF：** 词频，一个词在文档中出现的次数。TF越高，表示该词对当前文档越重要。
- **IDF：** 逆文档频率，一个衡量词语普遍重要性的指标。计算公式通常是 `log(总文档数 / 包含该词的文档数)`。如果一个词在太多文档中都出现（如“的”、“是”），其IDF值会很低，表明它区分文档的能力差。

**TF-IDF = TF × IDF**

**作用：** TF-IDF通过降低常见词的权重，提高稀有且重要词的权重，从而得到更好的文档向量表示。在电影评论中，“精彩绝伦”的TF-IDF值通常会远高于“电影”这个词。

#### 3. 词语级共现矩阵与词向量

这种方法旨在为**单个词语**生成有意义的向量表示，从而捕捉词语之间的语义关系。

**工作流程：**

1. **定义上下文窗口：** 选择一个固定大小的窗口（例如，窗口大小为1，即只看左右相邻的一个词）。
2. **构建共现矩阵：** 创建一个 |V| x |V| 的矩阵 X，其中 `X[i][j]` 表示在整个语料库中，词语 `j` 在词语 `i` 的上下文窗口中出现的次数。

**举例说明：**
语料库包含三个句子：

- “我 喜欢 电影”
- “我 喜欢 音乐”
- “我 享受 电影”

假设词汇表 V = `[我， 喜欢， 电影， 音乐， 享受]`，窗口大小为1。

我们构建共现矩阵（以“我”为例）：

- “我”的右边是“喜欢”和“享受”。
- 所以，在“我”的行中，“喜欢”计数+2，“享受”计数+1。

最终矩阵可能如下（仅示意）：

|          | 我   | 喜欢 | 电影 | 音乐 | 享受 |
| :------- | :--- | :--- | :--- | :--- | :--- |
| **我**   | 0    | 2    | 0    | 0    | 1    |
| **喜欢** | 2    | 0    | 1    | 1    | 0    |
| **电影** | 0    | 1    | 0    | 0    | 1    |
| **音乐** | 0    | 1    | 0    | 0    | 0    |
| **享受** | 1    | 0    | 1    | 0    | 0    |

**问题与优化：**

- 这个矩阵同样**高维且稀疏**。
- **解决方案：降维。** 使用诸如**主成分分析（PCA）** 或**奇异值分解（SVD）** 等技术，将这个巨大的稀疏矩阵压缩成一个低维的稠密矩阵（比如 |V| x 100）。这个新的、更小的矩阵的每一行，就是一个词语的**稠密词向量**。
- 在这些低维向量空间中，语义相似的词语（如“电影”和“影片”）其向量在空间中的位置会非常接近。

**GloVe模型** 就是基于这种思想的一个著名且成功的词向量模型，它本质上是对共现计数矩阵进行加权分解。





## 基于推理的方法